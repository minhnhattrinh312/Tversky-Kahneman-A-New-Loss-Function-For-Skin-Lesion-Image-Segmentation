{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"loss_functions.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"59UAX25VHT3D"},"source":["## Our New Tversky-Kahneman Loss"]},{"cell_type":"code","metadata":{"id":"v6H9wjOmDSLp"},"source":["def tversky_kahneman(target, output, gamma=0.279, smooth=1e-10):\n","  #output = tf.expand_dims(tf.argmax(output, axis=-1), axis = -1)\n","  #output = tf.cast(output, tf.float32)\n","  target = tf.cast(target, tf.float32)\n","  target_positive = K.flatten(target)\n","  output_positive = K.flatten(output)\n","\n","  true_pos = K.sum(target_positive * output_positive)\n","  true_neg = K.sum((1-target_positive) * (1-output_positive))\n","  false_neg = K.sum(target_positive * (1-output_positive))\n","  false_pos = K.sum((1-target_positive) * output_positive)\n","\n","  p = (true_pos + true_neg)/(true_pos + true_neg + false_pos + false_neg)   ###########\n","  p_gamma = K.pow(p,gamma) #p^gamma\n","  _p_gamma = K.pow(1-p, gamma) #(1-p)^gamma\n","  loss = _p_gamma/K.pow(p_gamma + _p_gamma, 1/gamma)\n","\n","  #print('tversky-kahneman: ', loss)\n","\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iioGGbxcuY9k"},"source":["## Traditional Pix2Pix generator loss and discriminator loss"]},{"cell_type":"code","metadata":{"id":"NxNlxEyWIUsS"},"source":["def discriminator_loss(disc_real_output, disc_generated_output):\n","  real_loss = tf.compat.v1.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_real_output), logits = disc_real_output)\n","  generated_loss = tf.compat.v1.losses.sigmoid_cross_entropy(multi_class_labels = tf.zeros_like(disc_generated_output), logits = disc_generated_output)\n","  total_disc_loss = real_loss + generated_loss\n","\n","  #print('real_loss: ', real_loss)\n","  #print('generated_loss: ', generated_loss)\n","\n","  return total_disc_loss\n","\n","def generator_loss(disc_generated_output, gen_output, target):\n","  gan_loss = tf.compat.v1.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_generated_output), logits = disc_generated_output) \n","  # mean absolute error\n","  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","  total_gen_loss = 0.01*gan_loss + l1_loss\n","  #print('gan loss: ', 0.01*gan_loss)\n","  #print('l1: ', l1_loss)\n","\n","  return total_gen_loss, gan_loss, l1_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lTW3j63uirZ"},"source":["## Levelset Loss and Active Contour Loss"]},{"cell_type":"code","metadata":{"id":"NYJ4GT81vKv7"},"source":["def levelsetLoss(target, output):\n","  loss = 0.0\n","  for ich in range(target.shape[3]):\n","    _target = target[...,ich:ich+1]    \n","    pcentroid = tf.reduce_sum(_target*output, (1,2), keepdims=True) / tf.reduce_sum(output, (1,2), keepdims=True)   \n","    plevel = _target - pcentroid\n","    pLoss = plevel*plevel*output\n","    loss += tf.reduce_sum(pLoss)\n","  return loss\n","\n","def gradientLoss(input, penalty=\"l1\"):\n","  dH = tf.math.abs(input[:, 1:, :, :] - input[:, :-1, :, :])\n","  dW = tf.math.abs(input[:, :, 1:, :] - input[:, :, :-1, :])\n","  if penalty == \"l2\":\n","    dH = dH * dH\n","    dW = dW * dW\n","\n","  loss =  tf.reduce_sum(dH) +  tf.reduce_sum(dW)\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjf3FQM1vwcY"},"source":["def pre_Active_Contour_Loss(target, output, outDim=2, smooth=0.001):  \n","  yTrueOnehot = tf.one_hot(tf.squeeze(tf.cast(target, tf.uint8), axis=-1), depth=outDim)\n","  #yTrueOnehot = tf.broadcast_to(yTrueOnehot, [1,256,256,1])\n","  #y_pred = tf.broadcast_to(output, [1,256,256,1])\n","  loss =  output*(1-yTrueOnehot) + (1-output)*yTrueOnehot\n","  return tf.reduce_mean(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgJ50ogN7MZo"},"source":["def Active_Contour_loss(input, target, output, alpha = 1e-7, beta = 1e-2):\n","  # print('\\ractive loss: ',self.activeContourLoss(trueLabel, output),' leverset loss: ', self.levelsetLoss(image,output),'gradient loss: ', self.gradientLoss(output), end=\"\")\n","  #print('pre-active contour: ', pre_Active_Contour_Loss(target, output))\n","  loss = alpha*(levelsetLoss(input, output) + beta*gradientLoss(output)) + pre_Active_Contour_Loss(target, output)\n","  #print('active contour: ', loss)\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a7IQCdrz_qs-"},"source":["## Dice Coefficient and Jaccard"]},{"cell_type":"code","metadata":{"id":"xLx59jHNL5yq"},"source":["def dice_coef(target, output, smooth=1e-10):\n","  #Average dice coefficient per batch\n","  axes = (1,2,3)\n","  intersection = K.sum(target*output, axis=axes)\n","  summation = K.sum(target+output, axis=axes)\n","  dice = K.mean((2.0*intersection+smooth)/(summation+smooth), axis=0)\n","  return dice\n","\n","def jaccard_coef(target, output, smooth=1e-10):\n","  #Average jaccard coefficient per batch\n","  axes = (1,2,3)\n","  intersection = K.sum(target*output, axis=axes)\n","  union = K.sum(target+output, axis=axes) - intersection\n","  jaccard = K.mean((intersection+smooth)/(union+smooth), axis=0)\n","  return jaccard\n","\n","def metrics(target, output):\n","  output_standard = tf.expand_dims(tf.argmax(output, axis=-1), axis = -1)     ########\n","  _output =  tf.where(output_standard == 1, 1.0, 0.0)    ############ \n","  _target =  tf.where(target == 1, 1.0, 0.0)\n","  #output = tf.broadcast_to(output, [1,256,256,1])\n","  #target = tf.broadcast_to(target, [1,256,256,1])\n","\n","  _dice_coef = dice_coef(_target, _output, smooth=1e-10)\n","  _jaccard_coef = jaccard_coef(_target, _output, smooth=1e-10)\n","  return _dice_coef, _jaccard_coef"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vuJuD94PCqx2"},"source":["## MVN"]},{"cell_type":"code","metadata":{"id":"pJEsYUuOCsPB"},"source":["def mvn(tensor, eps=1e-6):\n","  '''Performs per-channel spatial mean-variance normalization.'''\n","  mean = K.mean(tensor, axis=(1,2), keepdims=True)\n","  std = K.std(tensor, axis=(1,2), keepdims=True)\n","  mvn = (tensor - mean)/(std + eps)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BfGFa9SKC_RW"},"source":["## Precision and Recall"]},{"cell_type":"code","metadata":{"id":"bkW5Uh5sDDSd"},"source":["def confusion(target, output, smooth=1e-10):\n","  output_positive = K.clip(output, 0, 1)\n","  output_negative = 1 - output_positive\n","  target_positive = K.clip(target, 0, 1)\n","  target_negative = 1 - target_positive\n","  TP = K.sum(target_positive * output_positive)\n","  FP = K.sum(target_negative * output_positive)\n","  FN = K.sum(target_positive * output_negative) \n","  precision = (TP + smooth)/(TP + FP + smooth)\n","  recall = (TP + smooth)/(TP + FN + smooth)\n","  return precision, recall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdHmxGlQ9axH"},"source":["def TP(target, output, smooth=1e-10):\n","  output_positive = K.round(K.clip(output, 0, 1))\n","  target_positive = K.round(K.clip(target, 0, 1))\n","  TP = (K.sum(target_positive * output_positive) + smooth)/ (K.sum(target_positive) + smooth) \n","  return TP\n","\n","def TN(target, output, smooth=1e-10):\n","  smooth = 1\n","  output_positive = K.round(K.clip(output, 0, 1))\n","  output_negative = 1 - output_positive\n","  target_positive = K.clip(target, 0, 1)\n","  target_negative = 1 - target_positive\n","  TN = (K.sum(target_negative * output_negative) + smooth) / (K.sum(target_negative) + smooth)\n","  return TN"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-E1r6piP9dBH"},"source":["## Tversky Loss"]},{"cell_type":"code","metadata":{"id":"KAu6uXQjFBIG"},"source":["def tversky(target, output, smooth=1e-10):\n","  y_true_pos = K.flatten(target)\n","  y_pred_pos = K.flatten(output)\n","\n","  true_pos = K.sum(y_true_pos * y_pred_pos)\n","  false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n","  false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n","  alpha = 0.7\n","\n","  loss = (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n","\n","  return loss\n","\n","def tversky_loss(target, output, smooth=1e-10):\n","  return 1 - tversky(target, output, smooth=1e-10)\n","\n","def focal_tversky(target, output, smooth=1e-10):\n","  p = tversky(target, output, smooth=1e-10)\n","  gamma = 0.75\n","  \n","  return K.pow((1-p), gamma)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"05Icn3WuJ5zH"},"source":["## Accuracy"]},{"cell_type":"code","metadata":{"id":"FLgljQ21MM_0"},"source":["def accuracy(target, output, smooth=1e-10):\n","  target_positive = K.flatten(target)\n","  output_positive = K.flatten(output)\n","\n","  true_pos = K.sum(target_positive * output_positive)\n","  true_neg = K.sum((1-target_positive) * (1-output_positive))\n","  false_neg = K.sum(target_positive * (1-output_positive))\n","  false_pos = K.sum((1-target_positive) * output_positive)\n","\n","  p = (true_pos + true_neg + smooth)/(true_pos + true_neg + false_pos + false_neg + smooth)\n","  return p"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6EpJDtUdMN6m"},"source":["## SEN - SPE"]},{"cell_type":"code","metadata":{"id":"ybH14z9AMfhk"},"source":["def sen_spe(target, output, smooth=1e-10):\n","  target_positive = K.flatten(target)\n","  output_positive = K.flatten(output)\n","\n","  true_pos = K.sum(target_positive * output_positive)\n","  true_neg = K.sum((1-target_positive) * (1-output_positive))\n","  false_neg = K.sum(target_positive * (1-output_positive))\n","  false_pos = K.sum((1-target_positive) * output_positive)\n","\n","  sen = true_pos/(true_pos + false_neg)\n","  spe = true_neg/(true_neg + false_pos)\n","  return sen, spe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gYJJrV_tMQRS"},"source":["## Cross-Entropy"]},{"cell_type":"code","metadata":{"id":"hBasHx04Fri2"},"source":["def weighted_cross_entropy(target, output, beta):\n","  output_standard = tf.expand_dims(tf.argmax(output, axis=-1), axis = -1)     ########\n","  _output =  tf.where(output_standard == 1, 1.0, 0.0)    ############ \n","  _target =  tf.where(target == 1, 1.0, 0.0)\n","\n","  weight_a = beta * tf.cast(_target, tf.float32)\n","  weight_b = 1 - tf.cast(_target, tf.float32)\n","    \n","  o = (tf.math.log1p(tf.exp(-tf.abs(_output))) + tf.nn.relu(-_output)) * (weight_a + weight_b) + _output * weight_b \n","  loss = tf.reduce_mean(o)\n","\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zp450xJFGaeG"},"source":["def balanced_cross_entropy(y_true, y_pred, beta):\n","  weight_a = beta * tf.cast(y_true, tf.float32)\n","  weight_b = (1 - beta) * tf.cast(1 - y_true, tf.float32)\n","    \n","  o = (tf.math.log1p(tf.exp(-tf.abs(y_pred))) + tf.nn.relu(-y_pred)) * (weight_a + weight_b) + y_pred * weight_b\n","  loss = tf.reduce_mean(o)\n","\n","  return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AZfOYdKupvAs"},"source":["def binary_cross_entropy(y_true, y_pred):\n","  pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.zeros_like(y_pred))\n","  pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.ones_like(y_pred))\n","  loss = K.binary_crossentropy(y_true, y_pred)*pt_1 + K.binary_crossentropy(y_true, y_pred)*pt_0\n","\n","  return loss"],"execution_count":null,"outputs":[]}]}